\chapter{Создание кластерной системы}

В этой главе рассматриваются пошаговые действия при подготовке
автоматизированной установки кластерной системы из 3-ех нод,
описанной в предыдущей главе.

\section{Создание виртуальных сетей}

Для создания новой виртуальной сети в \textmd{virt-manager}
перейдем в "Правка"$\rightarrow$"Свойства подключения"$\rightarrow$"Виртуальные сети"
и в открывшемся диалоговом окне найдем кнопку по созданию сети.
Конфигурации сетей изображены на Рис. \ref{network-mpi} и
\ref{network-mpi-offline}.

\myImage{Конфигурация первой сети для доступа в Интернет}{network-mpi}{network-mpi}
\myImage{Конфигурация второй, внутренней сети}{network-mpi-offline}{network-mpi-offline}

\clearpage

\section{Получение основного kickstart-файла}

Следующим шагом, мы должны создать kickstart-файл, где будут
описаны все основные этапы установки системы. Kickstart-файл
представляет из себя простой текстовый файл (plain text)
который разбит на обязательные и опциональные секции. Создание
и отладка такого файла -- довольно трудоемкая задача. Для
упрощения его получения, можно провести тестовую установку
системы, после чего в домашней папке пользователя \textmd{root}
будет лежать файл \textmd{anaconda-ks.cfg}, сохронивший в
себе все этапы выбора при установке.

\myImage{Добавление второго сетевого интерфейса}{centos-vm}{centos-vm}

Для этого выберем пункт "Создать виртуальную машину" и
проследуем мастеру настройки, указав скаченный с официального сайта
образ CentOS и созданную сеть с доступом в Интернет. Также выберем пункт
"Дополнить конфигурацию перед установкой", чтобы добавить
второй интерфейс к доступу в изолированную сетью (Рис\ref{centos-vm}),
модель интерфейсов \textmd{virtio}, а для ускорения установки
в параметрах жесткого диска можно указать "unsafe" - кэширования.
\clearpage

\myImage{Процесс установки CentOS7}{centos-install}{centos-install}

После настройки физической конфигурации машины, мы можем нажать
"Начать установку" и перед нами раскроется окно с эмуляцией
монитора созданной машины. В ней мы увидим приглашение к установке
CentOS7, в которой можем произвести всю необходимую настройку
(Рис. \ref{centos-install}). После того, как мы
нажмем "Начать установку", мы сможем настроить пользователей
системы.

Наконец, перезагрузив систему и зайдя от \textmd{root}-пользователя
мы найдем файл \textmd{anaconda-ks.cfg}. При желании полученный
файл можно найти в приложении \ref{app:kickstart}.

\clearpage

\section{Генерация kickstart-файла с помощью python}
Для установки нескольких узлов, нам потребуется создать
копии полученного \textmd{kickstart}-файла. При этом
данные файлы будут отличаться только именем узла и
статическим IP-адрессом в изолированной сети. Мы можем
создать из полученного файла Jinja-шаблон\cite{jinja2}, и
сгенерировать полученные конфиги автоматически, в зависимости
от номера узла:

\verbatiminput{listings/generate.py}

А в шаблоне заменить изменяющиеся параметры:

\begin{verbatim}
# ...

network  --bootproto=static --device=eth1 --ip={{ip}} \
           --netmask=255.255.255.0 --ipv6=auto --activate
network  --hostname={{host}}

# ...
\end{verbatim}

После чего мы можем просто раздать http-сервером эти файлы.
И при установке дописав параметр, который запустит
автоматическую установку:
\begin{verbatim}
  inst.ks=http://192.168.127.1:8080/node1.cfg
\end{verbatim}

\section{Настройка NFS}

Рассмотрим настройку NFS-подсистемы, которая сделает общий
подкаталог \textmd{/nfs} для всех узлов. Данный критерий не
обязателен при работе с MPI, но очень упрощает разработку.

\subsection{Настройка NFS-сервера}

Для настройки nfs в CentOS есть пакеты \textmd{nfs-utils} и
\textmd{nfs-utils-lib}. В процессе установки
использовался DVD-образ CentOS7, и эти пакеты были выбраны
при установке. Поэтому остается только провести настройку
узлов:
\begin{verbatim}
mkdir -p /nfs # создадим общую
vim /etc/exports # правим конфигурацию nfs-сервера
\end{verbatim}
В /etc/exports вписываем следующую строку:
\begin{verbatim}
/nfs 10.20.30.0/24(rw,sync,no_root_squash,no_all_squash)
\end{verbatim}
При этом:
\begin{itemize}
  \item /home/nfs – расшариваемая директория;
  \item 10.20.30.40/24 – IP адрес клиента (или, как в моем случае, возможность подключения для всей подсети);
  \item rw – разрешение на запись;
  \item sync – синхронизация указанной директории;
  \item no\_root\_squash – включение root привилегий;
  \item no\_all\_squash — включение пользовательской авторизации;
\end{itemize}
После чего включаем все необходимые сервисы:
\begin{verbatim}
systemctl enable rpcbind
systemctl enable nfs-server
systemctl enable nfs-lock
systemctl enable nfs-idmap
systemctl start rpcbind
systemctl start nfs-server
systemctl start nfs-lock
systemctl start nfs-idmap
\end{verbatim}

\subsection{Настройка NFS-клиента}

Со стороны клиента требуется гораздо меньше забот. Достаточно
включить необобходимые сервисы:
\begin{verbatim}
systemctl enable rpcbind
systemctl enable nfs-server
systemctl enable nfs-lock
systemctl enable nfs-idmap
systemctl start rpcbind
systemctl start nfs-server
systemctl start nfs-lock
systemctl start nfs-idmap
\end{verbatim}
И примонтировать папку с сервера:
\begin{verbatim}
mkdir -p /nfs
mount -t nfs 10.20.30.41:/nfs /nfs
\end{verbatim}
После чего можно проверить работоспособность:
\begin{verbatim}
echo "Hello, NFS!" > /nfs/hello
# на node1 должен появится файл /nfs/hello
\end{verbatim}
И наконец, чтобы не монтировать NFS при каждом запуске,
подправим файл \textmd{/etc/nfs}:
\begin{verbatim}
# /etc/fstab
# ...
/dev/mapper/centos\_node2-root / xfs defaults 0 0
UUID=827da158-cf9b-44b9-9f1f-be86ccd21d49 /boot xfs defaults 0 0
/dev/mapper/centos\_node2-swap swap swap defaults   0 0
10.20.30.41:/nfs /nfs nfs rw,sync,hard,intr 0 0
\end{verbatim}
После чего проверяем коррекность внесенных правок: 
\begin{verbatim}
mount -fav
\end{verbatim}

\section{Установка Intel MPI}

Для установки Intel реализации MPI требуется пройти
регистрацию на их официальном сайте\cite{intel-mpi}, после
чего можно загрузить как коммерческую, так и бесплатную версию
для ознакомления (что мы и сделали).

Для полноценной работы с MPI нам потребуется поставить набор 
необходимых компиляторов: gfortran/gcc/gcc-c++, а также
сделать доступным все хосты по имени:
\begin{verbatim}
  # /etc/hosts
  # ...
  10.20.30.41 node1
  10.20.30.42 node2
  10.20.30.43 node3
\end{verbatim}

И так как пользоваться MPI должен обычный пользователь, мы
должны сделать доступным SSH-доступ к каждому узлу скопировав
публичный ключ:
\begin{verbatim}
  ssh-copy-id user@{node2,node3}
\end{verbatim}

Когда это сделано и архив загружен на рабочую станцию,
можно раздать его на виртуальные узлы:
\begin{verbatim}
  cd /nfs # установка потребуется на всех узлах
  wget http://192.168.127.1:8080/mpi.tgz
  tar -xf mpi.tgz 
\end{verbatim}

  Далее достаточно запустить процесс установки, отвечая 
на инструкции установочника (запустить на всех узлах):
\begin{verbatim}
  cd ./mpi
  ./install.sh 
\end{verbatim}

Теперь, для проверки, можно попробовать запустить на всех узлах 
какую-нибудь UNIX-программу. \textmd{hostname} для нас отлично
подойдет, так как мы  сможем легко убедимся в правильности 
запущенной программы:
\begin{verbatim}
  # Найдем скрипт, настраивающий переменные среды Intel MPI
  find / -type f -name 'mpivars.sh'
  
  # Запустим это user-пользователем
  # В последствии, можно добавить этот скрипт в /etc/.bashrc,
  # чтобы Intel MPI окружение настраивалось при входе
  . /opt/intel/compilers_and_libraries_2018.1.163/linux/mpi\\
    /intel64/bin/mpivars.sh
  
  # тестовый запуск
  # -ppn [число процессов на узел]
  # -n [число процессов в общем]
  # --hosts [список узлов через запятую]
  mpirun -ppn 1 -n 3 \
       -hosts 10.20.30.41,10.20.30.42,10.20.30.43 hostname
\end{verbatim}

Вывод программы (что и ожидалось):
\begin{verbatim}
  node1
  node2
  node3
\end{verbatim}

Для удобства использования, все хосты кластера задать в файле
\textmd{/nfs/hosts} и создать алиас в \textmd{.bashrc}-файле:
\begin{verbatim}
  alias mpirun='mpirun -f /nfs/hosts'
\end{verbatim}
После чего, параметр \textmd{--hosts} можно опустить: 
\begin{verbatim}
  mpirun -ppn 2 -n 6 hostname  
\end{verbatim}
\clearpage

\section{Запуск тестовой программы}

Для полноценной проверки работоспособности MPI запустим
тестовую программу, распространяемую вместе с Intel MPI
реализацией:
\begin{verbatim}
cd /nfs/user
# скопируем программу в рабочий котолог
cp -r /opt/intel/impi/2018.1.163/test/ ./
cd test

# скомпилируем программу, написанную на C
mpicc -o test test.c

# запустим скомпилированную программу
mpirun -ppn 2 -n 6 ./test
\end{verbatim}

Каждый узел этой программы отправляет свой номер, кол-во 
процессов и имя узла на 0-вой узел, который в свою очередь
производит печать поулченных данных. Таким образом
вывод нашей программы:
\begin{verbatim}
Hello world: rank 0 of 6 running on node1
Hello world: rank 1 of 6 running on node1
Hello world: rank 2 of 6 running on node2
Hello world: rank 3 of 6 running on node2
Hello world: rank 4 of 6 running on node3
Hello world: rank 5 of 6 running on node3
\end{verbatim}

Добавив пройденные шаги в секцию окончания установки шаблона
kickstart-файла, мы получим автоматизированно развертываемую
систему с преднастроеным пользователем и Intel MPI. 
В дальнейшем, функционал можно расширить за счет 
увеличения количества узлов, преднастройки
пользователей, переноса домашней директории пользователей
в nfs раздел и установки других реализаций MPI(В частности Open MPI).